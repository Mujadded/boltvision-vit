{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "0.15.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "\n",
    "pretrianed_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# for parameter in pretrianed_vit.parameters():\n",
    "#     parameter.requires_grad = False\n",
    "\n",
    "pretrianed_vit.heads = nn.Linear(in_features=768, out_features=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/train'), PosixPath('data/val'), PosixPath('data/test'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Preparing data for vit model\n",
    "from pathlib import Path\n",
    "\n",
    "# Setting up dataset path\n",
    "data_path = Path('data')\n",
    "# Setup train val data\n",
    "train_dir = data_path / 'train'\n",
    "val_dir = data_path / 'val'\n",
    "test_dir = data_path / 'test'\n",
    "\n",
    "train_dir, val_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Data augmentation we will do\n",
    "vit_transforms = pretrained_vit_weights.transforms()\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=(0, 90)),\n",
    "    vit_transforms\n",
    "])\n",
    "\n",
    "\n",
    "# Implementing test transforms with basic augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    vit_transforms\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_data = datasets.ImageFolder(val_dir, transform=test_transform)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "class_names = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Dataset into dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup batch size and number of workers \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['module.class_token', 'module.conv_proj.weight', 'module.conv_proj.bias', 'module.encoder.pos_embedding', 'module.encoder.layers.encoder_layer_0.ln_1.weight', 'module.encoder.layers.encoder_layer_0.ln_1.bias', 'module.encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_0.ln_2.weight', 'module.encoder.layers.encoder_layer_0.ln_2.bias', 'module.encoder.layers.encoder_layer_0.mlp.0.weight', 'module.encoder.layers.encoder_layer_0.mlp.0.bias', 'module.encoder.layers.encoder_layer_0.mlp.3.weight', 'module.encoder.layers.encoder_layer_0.mlp.3.bias', 'module.encoder.layers.encoder_layer_1.ln_1.weight', 'module.encoder.layers.encoder_layer_1.ln_1.bias', 'module.encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_1.ln_2.weight', 'module.encoder.layers.encoder_layer_1.ln_2.bias', 'module.encoder.layers.encoder_layer_1.mlp.0.weight', 'module.encoder.layers.encoder_layer_1.mlp.0.bias', 'module.encoder.layers.encoder_layer_1.mlp.3.weight', 'module.encoder.layers.encoder_layer_1.mlp.3.bias', 'module.encoder.layers.encoder_layer_2.ln_1.weight', 'module.encoder.layers.encoder_layer_2.ln_1.bias', 'module.encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_2.ln_2.weight', 'module.encoder.layers.encoder_layer_2.ln_2.bias', 'module.encoder.layers.encoder_layer_2.mlp.0.weight', 'module.encoder.layers.encoder_layer_2.mlp.0.bias', 'module.encoder.layers.encoder_layer_2.mlp.3.weight', 'module.encoder.layers.encoder_layer_2.mlp.3.bias', 'module.encoder.layers.encoder_layer_3.ln_1.weight', 'module.encoder.layers.encoder_layer_3.ln_1.bias', 'module.encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_3.ln_2.weight', 'module.encoder.layers.encoder_layer_3.ln_2.bias', 'module.encoder.layers.encoder_layer_3.mlp.0.weight', 'module.encoder.layers.encoder_layer_3.mlp.0.bias', 'module.encoder.layers.encoder_layer_3.mlp.3.weight', 'module.encoder.layers.encoder_layer_3.mlp.3.bias', 'module.encoder.layers.encoder_layer_4.ln_1.weight', 'module.encoder.layers.encoder_layer_4.ln_1.bias', 'module.encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_4.ln_2.weight', 'module.encoder.layers.encoder_layer_4.ln_2.bias', 'module.encoder.layers.encoder_layer_4.mlp.0.weight', 'module.encoder.layers.encoder_layer_4.mlp.0.bias', 'module.encoder.layers.encoder_layer_4.mlp.3.weight', 'module.encoder.layers.encoder_layer_4.mlp.3.bias', 'module.encoder.layers.encoder_layer_5.ln_1.weight', 'module.encoder.layers.encoder_layer_5.ln_1.bias', 'module.encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_5.ln_2.weight', 'module.encoder.layers.encoder_layer_5.ln_2.bias', 'module.encoder.layers.encoder_layer_5.mlp.0.weight', 'module.encoder.layers.encoder_layer_5.mlp.0.bias', 'module.encoder.layers.encoder_layer_5.mlp.3.weight', 'module.encoder.layers.encoder_layer_5.mlp.3.bias', 'module.encoder.layers.encoder_layer_6.ln_1.weight', 'module.encoder.layers.encoder_layer_6.ln_1.bias', 'module.encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_6.ln_2.weight', 'module.encoder.layers.encoder_layer_6.ln_2.bias', 'module.encoder.layers.encoder_layer_6.mlp.0.weight', 'module.encoder.layers.encoder_layer_6.mlp.0.bias', 'module.encoder.layers.encoder_layer_6.mlp.3.weight', 'module.encoder.layers.encoder_layer_6.mlp.3.bias', 'module.encoder.layers.encoder_layer_7.ln_1.weight', 'module.encoder.layers.encoder_layer_7.ln_1.bias', 'module.encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_7.ln_2.weight', 'module.encoder.layers.encoder_layer_7.ln_2.bias', 'module.encoder.layers.encoder_layer_7.mlp.0.weight', 'module.encoder.layers.encoder_layer_7.mlp.0.bias', 'module.encoder.layers.encoder_layer_7.mlp.3.weight', 'module.encoder.layers.encoder_layer_7.mlp.3.bias', 'module.encoder.layers.encoder_layer_8.ln_1.weight', 'module.encoder.layers.encoder_layer_8.ln_1.bias', 'module.encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_8.ln_2.weight', 'module.encoder.layers.encoder_layer_8.ln_2.bias', 'module.encoder.layers.encoder_layer_8.mlp.0.weight', 'module.encoder.layers.encoder_layer_8.mlp.0.bias', 'module.encoder.layers.encoder_layer_8.mlp.3.weight', 'module.encoder.layers.encoder_layer_8.mlp.3.bias', 'module.encoder.layers.encoder_layer_9.ln_1.weight', 'module.encoder.layers.encoder_layer_9.ln_1.bias', 'module.encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_9.ln_2.weight', 'module.encoder.layers.encoder_layer_9.ln_2.bias', 'module.encoder.layers.encoder_layer_9.mlp.0.weight', 'module.encoder.layers.encoder_layer_9.mlp.0.bias', 'module.encoder.layers.encoder_layer_9.mlp.3.weight', 'module.encoder.layers.encoder_layer_9.mlp.3.bias', 'module.encoder.layers.encoder_layer_10.ln_1.weight', 'module.encoder.layers.encoder_layer_10.ln_1.bias', 'module.encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_10.ln_2.weight', 'module.encoder.layers.encoder_layer_10.ln_2.bias', 'module.encoder.layers.encoder_layer_10.mlp.0.weight', 'module.encoder.layers.encoder_layer_10.mlp.0.bias', 'module.encoder.layers.encoder_layer_10.mlp.3.weight', 'module.encoder.layers.encoder_layer_10.mlp.3.bias', 'module.encoder.layers.encoder_layer_11.ln_1.weight', 'module.encoder.layers.encoder_layer_11.ln_1.bias', 'module.encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'module.encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'module.encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'module.encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'module.encoder.layers.encoder_layer_11.ln_2.weight', 'module.encoder.layers.encoder_layer_11.ln_2.bias', 'module.encoder.layers.encoder_layer_11.mlp.0.weight', 'module.encoder.layers.encoder_layer_11.mlp.0.bias', 'module.encoder.layers.encoder_layer_11.mlp.3.weight', 'module.encoder.layers.encoder_layer_11.mlp.3.bias', 'module.encoder.ln.weight', 'module.encoder.ln.bias', 'module.heads.weight', 'module.heads.bias'], unexpected_keys=['epoch', 'best_fitness', 'model', 'optimizer', 'hyp', 'date'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrianed_vit = torch.nn.DataParallel(pretrianed_vit)\n",
    "pretrianed_vit.load_state_dict(torch.load('runs/pretrained_vit_b16/20231127-184722/models/best.pt'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making predictions: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_trainer.engine import test\n",
    "optimizer = torch.optim.Adam(pretrianed_vit.parameters(),\n",
    "                            lr=1e-3,\n",
    "                            betas=(0.9,0.999),\n",
    "                            weight_decay=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "test_hash = test(model=pretrianed_vit, test_dataloader=test_dataloader,\n",
    "                             loss_fn=loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "meta = {'epochs': 10, 'val_acc': None,\n",
    "                    'test_acc': f'{test_hash[\"test_accuracy\"]*100:.2f}%',\n",
    "                    'test_loss': test_hash['test_loss'],\n",
    "                    'date': datetime.now().isoformat()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "generated_report = classification_report(test_dataloader.dataset.targets,test_hash['prediction_tensors'].numpy(), target_names=class_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'missing': {'precision': 0.7272727272727273, 'recall': 0.8, 'f1-score': 0.761904761904762, 'support': 10.0}, 'present': {'precision': 0.6, 'recall': 0.5, 'f1-score': 0.5454545454545454, 'support': 6.0}, 'accuracy': 0.6875, 'macro avg': {'precision': 0.6636363636363636, 'recall': 0.65, 'f1-score': 0.6536796536796536, 'support': 16.0}, 'weighted avg': {'precision': 0.6795454545454546, 'recall': 0.6875, 'f1-score': 0.6807359307359307, 'support': 16.0}}\n"
     ]
    }
   ],
   "source": [
    "print(generated_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7272727272727273,\n",
       " 'recall': 0.8,\n",
       " 'f1-score': 0.761904761904762,\n",
       " 'support': 10.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_report['missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6,\n",
       " 'recall': 0.5,\n",
       " 'f1-score': 0.5454545454545454,\n",
       " 'support': 6.0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_report['present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6636363636363636,\n",
       " 'recall': 0.65,\n",
       " 'f1-score': 0.6536796536796536,\n",
       " 'support': 16.0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_report['macro avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6795454545454546,\n",
       " 'recall': 0.6875,\n",
       " 'f1-score': 0.6807359307359307,\n",
       " 'support': 16.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_report['weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([{'precision': 0.7272727272727273, 'recall': 0.8, 'f1-score': 0.761904761904762, 'support': 10.0}, {'precision': 0.6, 'recall': 0.5, 'f1-score': 0.5454545454545454, 'support': 6.0}, 0.6875, {'precision': 0.6636363636363636, 'recall': 0.65, 'f1-score': 0.6536796536796536, 'support': 16.0}, {'precision': 0.6795454545454546, 'recall': 0.6875, 'f1-score': 0.6807359307359307, 'support': 16.0}])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_report.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_report['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = generated_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, vals = list(metrics.keys()), list(metrics.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_exist = False\n",
    "for key in keys:\n",
    "    if key == 'accuracy':\n",
    "        continue\n",
    "    row_heads = list(metrics[key].keys())\n",
    "    row_values = list(metrics[key].values())\n",
    "    n=len(row_heads) + 1\n",
    "    s = '' if header_exist else (('%23s,' * n % tuple(['class'] + row_heads)).rstrip(',') + '\\n')  # header\n",
    "    header_exist = True\n",
    "    with open('test.csv', 'a') as f:\n",
    "                f.write(s + ('%23s,' * n % tuple([key] + row_values)).rstrip(',') + '\\n')\n",
    "with open('test.csv', 'a') as f:\n",
    "    f.write(('%23s,' * n % tuple(['accuracy'] + [str(metrics['accuracy'])] + [' ' for i in range(n-2)])).rstrip(',') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'               accuracy,                 0.6875,                       ,                       ,                       ,'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('missing', {'precision': 0.7272727272727273, 'recall': 0.8, 'f1-score': 0.761904761904762, 'support': 10.0}), ('present', {'precision': 0.6, 'recall': 0.5, 'f1-score': 0.5454545454545454, 'support': 6.0}), ('accuracy', 0.6875), ('macro avg', {'precision': 0.6636363636363636, 'recall': 0.65, 'f1-score': 0.6536796536796536, 'support': 16.0}), ('weighted avg', {'precision': 0.6795454545454546, 'recall': 0.6875, 'f1-score': 0.6807359307359307, 'support': 16.0})])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing\n",
      "present\n",
      "accuracy\n",
      "macro avg\n",
      "weighted avg\n"
     ]
    }
   ],
   "source": [
    "for k, v in metrics.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92e67d5ffbed6dc58e9f7fa1e06c72a68fbdda23e0cf40b8f772bb6639a6f8df"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('pytorch_cuda_11.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
